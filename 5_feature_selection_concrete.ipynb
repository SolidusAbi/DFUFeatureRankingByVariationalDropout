{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from dataset import FeaturesDataset\n",
    "import numpy as np\n",
    "import os, sys\n",
    "\n",
    "project_root_dir = os.path.dirname('.')\n",
    "sparse_dir = os.path.join(project_root_dir, 'modules/Sparse')\n",
    "if sparse_dir not in sys.path:\n",
    "    sys.path.append(sparse_dir)\n",
    "\n",
    "dataset = FeaturesDataset(dataset_dir='data/dataset/oversampling', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concrete Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from Sparse.modules.variational import VariationalLayer\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class FeatureSelectionConcreteDropout(nn.Module):\n",
    "    def __init__(self, in_features: int, p_threshold:float = 0.1, init_min=.5, init_max=.5) -> None:\n",
    "        super(FeatureSelectionConcreteDropout, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.logit_threshold = np.log(p_threshold) - np.log(1. - p_threshold)\n",
    "\n",
    "        logit_init_min = np.log(init_min) - np.log(1. - init_min)\n",
    "        logit_init_max = np.log(init_max) - np.log(1. - init_max)\n",
    "        self.logit_p = Parameter(torch.rand(in_features) * (logit_init_max - logit_init_min) + logit_init_min)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.training:\n",
    "            return self.concrete_bernoulli(x)\n",
    "\n",
    "        return x * (self.logit_p < self.logit_threshold).float()\n",
    "\n",
    "    def concrete_bernoulli(self, x):\n",
    "        eps = 1e-8\n",
    "        unif_noise = torch.cuda.FloatTensor(*x.size()).uniform_() if self.logit_p.is_cuda else torch.FloatTensor(*x.size()).uniform_()\n",
    "\n",
    "        p = torch.sigmoid(self.logit_p)\n",
    "        tmp = .5\n",
    "\n",
    "        drop_prob = (torch.log(p + eps) - torch.log((1-p) + eps) + torch.log(unif_noise + eps)\n",
    "        - torch.log((1. - unif_noise) + eps))\n",
    "        drop_prob = torch.sigmoid(drop_prob / tmp)\n",
    "\n",
    "        self._drop_prob = drop_prob\n",
    "\n",
    "        random_tensor = 1 - drop_prob\n",
    "        # retain_prob = 1 - p # rescale factor typical for dropout, not necessary here!\n",
    "\n",
    "        # return torch.mul(x,random_tensor)\n",
    "        return x * random_tensor\n",
    "\n",
    "        # return torch.mul(x, random_tensor) #/ retain_prob\n",
    "\n",
    "    def reg(self):\n",
    "        p = torch.sigmoid( self.logit_p )\n",
    "        return torch.mean(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,in_features: int, nb_features: int, threshold: float = .1):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        if threshold < 0. or threshold > 1.:\n",
    "            raise ValueError('threshold must be between 0 and 1')\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # LinearCD(in_features, nb_features, bias=True, threshold=threshold),\n",
    "            # nn.SiLU(),\n",
    "            FeatureSelectionConcreteDropout(in_features, p_threshold=threshold),\n",
    "            nn.Linear(in_features, nb_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_features, nb_features//2),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_features//2, nb_features//4),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_features//4, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(model, dataset, batch_size = 128, n_epochs=10, log_dir='log/fs_cd'):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    logger = SummaryWriter(log_dir)\n",
    "\n",
    "    # weighted sampler\n",
    "    samples = dataset.dataset.y[dataset.indices]\n",
    "    class_weight = [1/(samples == 0).sum(), 1/(samples == 1).sum()]\n",
    "    samples_weight = np.zeros(len(dataset))\n",
    "    samples_weight[samples == 0] = class_weight[0]\n",
    "    samples_weight[samples == 1] = class_weight[1]\n",
    "    \n",
    "    sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(dataset))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "    \n",
    "    reg = 1e-6\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "            range(n_epochs),\n",
    "            leave=True,\n",
    "            unit=\"epoch\",\n",
    "            postfix={\"tls\": \"%.4f\" % 1},\n",
    "        )\n",
    "\n",
    "    modules = []\n",
    "    for i in model.modules():\n",
    "        if isinstance(i, FeatureSelectionConcreteDropout):\n",
    "            modules.append(i)\n",
    "\n",
    "    for epoch in epoch_iterator:\n",
    "        # reg = min(reg + 5e-5, 1e-2)\n",
    "        reg = min(reg + 2.5e-3, 1)\n",
    "        logger.add_scalar('kl_reg', reg, epoch)\n",
    "        \n",
    "        train_loss, train_acc = 0, 0 \n",
    "        for idx, (inputs, targets) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            reg_value = 0\n",
    "            for module in modules:\n",
    "                reg_value += module.reg()\n",
    "\n",
    "            loss = criterion(outputs, targets) + reg*reg_value\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log\n",
    "            pred = outputs.data.max(1)[1]\n",
    "            train_loss += float(loss)\n",
    "            train_acc += np.sum(np.equal(pred.cpu().numpy(), targets.cpu().data.numpy()))\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                epoch_iterator.set_postfix(tls=\"%.4f\" % loss.item())\n",
    "    \n",
    "        logger.add_scalar('Loss', train_loss / len(dataset), epoch)\n",
    "        logger.add_scalar('Accuracy', train_acc / len(dataset) * 100, epoch)\n",
    "\n",
    "        for i, c in enumerate(model.model.children()):\n",
    "            if hasattr(c, 'reg'):\n",
    "                logger.add_scalar('sp_%s' % i, (c.logit_p.cpu().data.numpy() > c.logit_threshold).sum(), epoch)\n",
    "\n",
    "    print(reg)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_features = len(dataset.features)\n",
    "\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "features_importance = []\n",
    "model_accuracy = []\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    train_set = torch.utils.data.Subset(dataset, train_ids)\n",
    "    test_set = torch.utils.data.Subset(dataset, test_ids)\n",
    "\n",
    "    model = Model(n_features, 256, threshold=.9)\n",
    "    model = train(model, train_set, batch_size=32, n_epochs=500, log_dir='log/fs_cd/{}'.format(fold))\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "    for inputs, targets in test_loader:\n",
    "        pred = model(inputs.cuda() if torch.cuda.is_available() else inputs)\n",
    "        pred = pred.argmax(dim=1)\n",
    "        y_pred.append(pred.cpu())\n",
    "        y_true.append(targets)\n",
    "\n",
    "    y_pred = torch.cat(y_pred)\n",
    "    y_true = torch.cat(y_true)\n",
    "    model_accuracy.append(accuracy_score(y_true, y_pred))\n",
    "    features_importance.append(torch.sigmoid(model.model[0].logit_p).cpu().detach().numpy())\n",
    "    print(model_accuracy[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean accuracy\n",
    "print('Mean accuracy: {}'.format(np.mean(model_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance_ = torch.tensor(np.array(features_importance)).mean(axis=0)\n",
    "features_score, index = features_importance_.sort()\n",
    "features_names = np.array(dataset.features)[index.cpu()]\n",
    "\n",
    "features_importance_df = pd.DataFrame(features_importance_[index], index=features_names, columns=['Importance'])\n",
    "features_importance_df.index.name = 'Features'\n",
    "\n",
    "features_importance_df.to_csv('data/features_importance/oversampling/concrete_dropout.csv')\n",
    "\n",
    "features_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance_ = np.array(features_importance).mean(axis=0)\n",
    "\n",
    "features_score, index = torch.tensor(features_importance_).sort()\n",
    "\n",
    "features_names = dataset.features\n",
    "\n",
    "print('Features:{}'.format(np.array(features_names)[index]))\n",
    "print('Features Score:{}'.format(1-features_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.model[0]\n",
    "test.concrete_bernoulli(torch.rand(1, 125).cuda()) > .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "x, y = next(iter(loader))\n",
    "\n",
    "threshold = .1\n",
    "model.model[0].logit_threshold = torch.tensor(np.log(threshold) - np.log(1. - threshold))\n",
    "model.eval()\n",
    "torch.argmax(torch.softmax(model(x.cuda()), dim=1), dim=1)\n",
    "print(y==torch.argmax(torch.softmax(model(x.cuda()), dim=1), dim=1).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(dataset.features)[(torch.sigmoid(model.model[0].logit_p)<0.1).cpu()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d9b8aa8d774518be7ebcfd06a2463a8035a66798fac49b1a363f570d2d8622e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
