{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_root_dir = os.path.dirname('.')\n",
    "sparse_dir = os.path.join(project_root_dir, 'modules/Sparse')\n",
    "if sparse_dir not in sys.path:\n",
    "    sys.path.append(sparse_dir)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.parameter import Parameter\n",
    "# import torch.nn.functional as F\n",
    "# from Sparse.modules.variational import VariationalLayer\n",
    "\n",
    "# class LinearSVD(nn.Linear, VariationalLayer):\n",
    "#     def __init__(self, in_features, out_features, p_threshold = 0.952572, bias=True) -> None:\n",
    "#         r'''\n",
    "#             Parameters\n",
    "#             ----------\n",
    "#                 in_features: int,\n",
    "#                     Number of input features.\n",
    "\n",
    "#                 out_features: int,\n",
    "#                     Number of output features.\n",
    "                \n",
    "#                 p_threshold: float,\n",
    "#                     It consists in the \\rho (binary dropout rate) threshold used in order to discard the weight.\n",
    "#                     In this approach, an Gaussian Dropout is being used which std is \\alpha = \\rho/(1-\\rho) so, \n",
    "#                     Infinitely large \\sigma_{ij} corresponds to infinitely large multiplicative noise in w_{ij}. By \n",
    "#                     default, the threshold is set to 0.952572 (\\log(\\sigma) ~ 3).\n",
    "\n",
    "#                 bias: bool,\n",
    "#                     If True, adds a bias term to the output.\n",
    "#         '''\n",
    "#         super(LinearSVD, self).__init__(in_features, out_features, bias)\n",
    "    \n",
    "#         self.log_alpha_threshold = np.log(p_threshold / (1-p_threshold))\n",
    "#         self.log_sigma = Parameter(1e-2 * torch.randn(out_features, in_features))\n",
    "\n",
    "#         self.log_sigma.data.fill_(-5) # Initialization based on the paper, Figure 1\n",
    "    \n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         self.log_alpha = self.compute_log_alpha(self.log_sigma, torch.abs(self.weight))\n",
    "\n",
    "#         if self.training:\n",
    "#             # LRT = local reparametrization trick (For details, see https://arxiv.org/pdf/1506.02557.pdf)\n",
    "#             lrt_mean =  F.linear(x, self.weight, self.bias)\n",
    "#             lrt_std = torch.sqrt(F.linear(x * x, torch.exp(self.log_sigma * 2.0)) + 1e-8)#.unsqueeze(dim=1)\n",
    "#             eps = torch.normal(0, torch.ones_like(lrt_std))\n",
    "#             return lrt_mean + lrt_std * eps\n",
    "        \n",
    "#         return F.linear(x, self.weight * (self.log_alpha < self.log_alpha_threshold).float(), self.bias)\n",
    "\n",
    "#     def kl_reg(self):\n",
    "#         k1, k2, k3 = torch.Tensor([0.63576]).cuda(), torch.Tensor([1.8732]).cuda(), torch.Tensor([1.48695]).cuda()\n",
    "#         kl = k1 * torch.sigmoid(k2 + k3 * self.log_alpha) - 0.5 * torch.log1p(torch.exp(-self.log_alpha))\n",
    "#         return -(torch.sum(kl))\n",
    "\n",
    "#     def compute_log_alpha(self, log_sigma, theta):\n",
    "#         r''' \n",
    "#             Compute the log \\alpha values from \\theta and log \\sigma^2.\n",
    "\n",
    "#             The relationship between \\sigma^2, \\theta, and \\alpha as defined in the\n",
    "#             paper https://arxiv.org/abs/1701.05369 is \\sigma^2 = \\alpha * \\theta^2.\n",
    "\n",
    "#             This method calculates the log \\alpha values based on this relation:\n",
    "#                 \\log(\\alpha) = 2*\\log(\\sigma) - 2*\\log(\\theta)\n",
    "#         ''' \n",
    "#         log_alpha = log_sigma * 2.0 - 2.0 * torch.log(1e-16 + torch.abs(theta))\n",
    "#         log_alpha = torch.clamp(log_alpha, -10, 10) # clipping for a numerical stability\n",
    "#         return log_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_alpha(log_sigma, theta):\n",
    "        r''' \n",
    "            Compute the log \\alpha values from \\theta and log \\sigma^2.\n",
    "\n",
    "            The relationship between \\sigma^2, \\theta, and \\alpha as defined in the\n",
    "            paper https://arxiv.org/abs/1701.05369 is \\sigma^2 = \\alpha * \\theta^2.\n",
    "\n",
    "            This method calculates the log \\alpha values based on this relation:\n",
    "                \\log(\\alpha) = 2*\\log(\\sigma) - 2*\\log(\\theta)\n",
    "        ''' \n",
    "        log_alpha = log_sigma * 2.0 - 2.0 * torch.log(1e-16 + torch.abs(theta))\n",
    "        log_alpha = torch.clamp(log_alpha, -10, 10) # clipping for a numerical stability\n",
    "        return log_alpha\n",
    "\n",
    "from Sparse.modules.variational import VariationalLayer\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class FeatureSelectionVariationalDropout(nn.Module, VariationalLayer):\n",
    "    def __init__(self, in_features: int, p_threshold:float = 0.952572) -> None:\n",
    "        super(FeatureSelectionVariationalDropout, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.log_alpha_threshold = np.log(p_threshold / (1-p_threshold))\n",
    "        self.log_sigma = Parameter(1e-2 * torch.randn(in_features))\n",
    "        self.weight = Parameter(torch.randn(in_features))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.log_alpha = compute_log_alpha(self.log_sigma, torch.abs(self.weight))\n",
    "        \n",
    "        if self.training:\n",
    "            # LRT = local reparametrization trick (For details, see https://arxiv.org/pdf/1506.02557.pdf)\n",
    "            lrt_mean =  x * self.weight\n",
    "            lrt_std = torch.sqrt((x*x) * torch.exp(self.log_sigma * 2.0) + 1e-8)\n",
    "            eps = torch.normal(0, torch.ones_like(lrt_std))\n",
    "            return lrt_mean + lrt_std * eps\n",
    "        \n",
    "        return x * self.weight * (self.log_alpha < self.log_alpha_threshold).float()\n",
    "\n",
    "    def kl_reg(self):\n",
    "        k1, k2, k3 = torch.Tensor([0.63576]).cuda(), torch.Tensor([1.8732]).cuda(), torch.Tensor([1.48695]).cuda()\n",
    "        kl = k1 * torch.sigmoid(k2 + k3 * self.log_alpha) - 0.5 * torch.log1p(torch.exp(-self.log_alpha))\n",
    "        return -(torch.sum(kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sparse.modules.variational.utils import SGVBL\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,in_features: int, nb_features: int, threshold: float = .95):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        if threshold < 0. or threshold > 1.:\n",
    "            raise ValueError('threshold must be between 0 and 1')\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            FeatureSelectionVariationalDropout(in_features, p_threshold=threshold),\n",
    "            nn.Linear(in_features, nb_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_features, nb_features//2),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_features//2, nb_features//4),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_features//4, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import FeaturesDataset\n",
    "import numpy as np\n",
    "\n",
    "dataset = FeaturesDataset('data/dataset/', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(model, dataset, batch_size = 128, n_epochs=10, log_dir='log/fs_vd'):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = SGVBL(model, len(dataset)).to(device)\n",
    "\n",
    "    logger = SummaryWriter(log_dir)\n",
    "\n",
    "    # weighted sampler\n",
    "    samples = dataset.dataset.y[dataset.indices]\n",
    "    class_weight = [1/(samples == 0).sum(), 1/(samples == 1).sum()]\n",
    "    samples_weight = np.zeros(len(dataset))\n",
    "    samples_weight[samples == 0] = class_weight[0]\n",
    "    samples_weight[samples == 1] = class_weight[1]\n",
    "    \n",
    "    sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(dataset))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "    reg = 1e-6\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "            range(n_epochs),\n",
    "            leave=True,\n",
    "            unit=\"epoch\",\n",
    "            postfix={\"tls\": \"%.4f\" % 1},\n",
    "        )\n",
    "\n",
    "    for epoch in epoch_iterator:\n",
    "        reg = min(reg + 2.5e-3, 1)\n",
    "        logger.add_scalar('kl_reg', reg, epoch)\n",
    "\n",
    "        train_loss, train_acc = 0, 0 \n",
    "        for idx, (inputs, targets) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets, reg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log\n",
    "            pred = outputs.data.max(1)[1]\n",
    "            train_loss += float(loss)\n",
    "\n",
    "            train_acc += np.sum(np.equal(pred.cpu().numpy(), targets.cpu().data.numpy()))\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                epoch_iterator.set_postfix(tls=\"%.4f\" % loss.item())\n",
    "        \n",
    "        logger.add_scalar('Loss', train_loss / len(dataset), epoch)\n",
    "        logger.add_scalar('Accuracy', train_acc / len(dataset) * 100, epoch)\n",
    "\n",
    "        for i, c in enumerate(model.model.children()):\n",
    "            if hasattr(c, 'kl_reg'):\n",
    "                logger.add_scalar('sp_%s' % i, (c.log_alpha.cpu().data.numpy() > c.log_alpha_threshold).sum(), epoch)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_features = len(dataset.features)\n",
    "\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "features_importance = []\n",
    "model_accuracy = []\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    train_set = torch.utils.data.Subset(dataset, train_ids)\n",
    "    test_set = torch.utils.data.Subset(dataset, test_ids)\n",
    "\n",
    "    model = Model(n_features, 256, threshold=.9)\n",
    "    model = train(model, train_set, batch_size=32, n_epochs=500, log_dir='log/fs_vd/{}'.format(fold))\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "    for inputs, targets in test_loader:\n",
    "        pred = model(inputs.cuda())\n",
    "        pred = pred.argmax(dim=1)\n",
    "        y_pred.append(pred.cpu())\n",
    "        y_true.append(targets)\n",
    "\n",
    "    y_pred = torch.cat(y_pred)\n",
    "    y_true = torch.cat(y_true)\n",
    "    model_accuracy.append(accuracy_score(y_true, y_pred))\n",
    "    features_importance.append(torch.sigmoid(model.model[0].log_alpha).cpu().detach().numpy())\n",
    "    print(model_accuracy[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean accuracy\n",
    "print('Mean accuracy: {}'.format(np.mean(model_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "features_importance_ = torch.tensor(np.array(features_importance)).mean(axis=0)\n",
    "features_score, index = features_importance_.sort()\n",
    "features_names = np.array(dataset.features)[index.cpu()]\n",
    "\n",
    "features_importance_df = pd.DataFrame(features_importance_[index], index=features_names, columns=['Importance'])\n",
    "features_importance_df.index.name = 'Features'\n",
    "\n",
    "features_importance_df.to_csv('data/features_importance/variational_dropout.csv')\n",
    "\n",
    "features_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d9b8aa8d774518be7ebcfd06a2463a8035a66798fac49b1a363f570d2d8622e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
