{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from dataset import FeaturesDataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = FeaturesDataset(dataset_dir='data/dataset/oversampling', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_df = pd.read_csv('data/dataset/dfu_features_dataset_selected.csv', index_col=0)\n",
    "# y_df = pd.read_csv('data/dataset/dfu_labels_dataset.csv', index_col=0)\n",
    "\n",
    "X_df = pd.read_csv('data/dataset/oversampling/dfu_features_dataset_selected.csv')\n",
    "y_df = pd.read_csv('data/dataset/oversampling/dfu_labels_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Feature Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# X = X_df.to_numpy()\n",
    "# y = y_df.to_numpy()\n",
    "\n",
    "X = dataset.X\n",
    "y = dataset.y\n",
    "\n",
    "features_importance = []\n",
    "model_accuracy = []\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=250, random_state=42, n_jobs=-1)\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X)):\n",
    "    print('Fold: {}'.format(fold))\n",
    "    X_train = X[train_ids]\n",
    "    y_train = y[train_ids]\n",
    "\n",
    "    X_test = X[test_ids]\n",
    "    y_test = y[test_ids]\n",
    "\n",
    "    # use RandomForestClassifier for feature ranking\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    # get importance\n",
    "    features_importance.append(clf.feature_importances_) \n",
    "\n",
    "    # testing\n",
    "    y_pred = clf.predict(X_test)\n",
    "    model_accuracy.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean accuracy\n",
    "print('Mean accuracy: {}'.format(np.mean(model_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features importance in a CSV file\n",
    "features_importance_ = np.array(features_importance)\n",
    "features_importance_ = np.mean(features_importance_, axis=0)\n",
    "\n",
    "# Sort the features importance and save on csv file with the feature name\n",
    "sort_idx = np.argsort(features_importance_)[::-1]\n",
    "features_importance_ = features_importance_[sort_idx]\n",
    "features_name = np.array(dataset.features)[sort_idx]\n",
    "\n",
    "features_importance_df = pd.DataFrame(features_importance_, index=features_name, columns=['Importance'])\n",
    "features_importance_df.index.name = 'Features'\n",
    "\n",
    "features_importance_df.to_csv('data/features_importance/oversampling/random_forest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection by Lasso, classificatiom\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "X = dataset.X\n",
    "y = dataset.y\n",
    "\n",
    "features_importance = []\n",
    "model_accuracy = []\n",
    "\n",
    "clf = LogisticRegression(penalty='l1', random_state=42, solver='liblinear')\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X)):\n",
    "    print('Fold: {}'.format(fold))\n",
    "    X_train = X[train_ids]\n",
    "    y_train = y[train_ids]\n",
    "\n",
    "    X_test = X[test_ids]\n",
    "    y_test = y[test_ids]\n",
    "\n",
    "    # use RandomForestClassifier for feature ranking\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    # get importance\n",
    "    features_importance.append(clf.coef_) \n",
    "\n",
    "    # testing\n",
    "    y_pred = clf.predict(X_test)\n",
    "    model_accuracy.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean accuracy\n",
    "print('Mean accuracy: {}'.format(np.mean(model_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features importance in a CSV file\n",
    "features_importance_ = np.array(features_importance)\n",
    "features_importance_ = np.mean(features_importance_, axis=0)\n",
    "features_importance_ = np.abs(features_importance_).ravel()\n",
    "\n",
    "# Sort the features importance and save on csv file with the feature name\n",
    "sort_idx = np.argsort(features_importance_)[::-1]\n",
    "features_importance_ = features_importance_[sort_idx]\n",
    "# features_name = X_df.columns[sort_idx]\n",
    "features_name = np.array(dataset.features)[sort_idx]\n",
    "\n",
    "\n",
    "features_importance_df = pd.DataFrame(features_importance_, index=features_name, columns=['Importance'])\n",
    "features_importance_df.index.name = 'Features'\n",
    "\n",
    "features_importance_df.to_csv('data/features_importance/oversampling/lasso.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-Squared for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with possible negative values for the chi2 test\n",
    "# X_df = X_df + np.abs(X_df.min().min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-squared for feature selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Necessary to remove negative values, simple normalization\n",
    "X = dataset.X\n",
    "X = (X - X.min(axis=0))\n",
    "X = X / X.max(axis=0)\n",
    "y = dataset.y\n",
    "\n",
    "features_importance = []\n",
    "model_accuracy = []\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X)):\n",
    "    print('Fold: {}'.format(fold))\n",
    "    X_train = X[train_ids]\n",
    "    y_train = y[train_ids]\n",
    "\n",
    "    X_test = X[test_ids]\n",
    "    y_test = y[test_ids]\n",
    "\n",
    "    # use RandomForestClassifier for feature ranking\n",
    "    clf = SelectKBest(chi2, k=10)\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    # get importance\n",
    "    features_importance.append(clf.scores_) \n",
    "\n",
    "    # testing\n",
    "    X_train = clf.transform(X_train)\n",
    "    X_test = clf.transform(X_test)\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "    y_pred = clf.predict(X_test)\n",
    "    model_accuracy.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features importance in a CSV file\n",
    "features_importance_ = np.array(features_importance)\n",
    "features_importance_ = np.mean(features_importance_, axis=0)\n",
    "features_importance_ = np.abs(features_importance_).ravel()\n",
    "\n",
    "# Sort the features importance and save on csv file with the feature name\n",
    "sort_idx = np.argsort(features_importance_)[::-1]\n",
    "features_importance_ = features_importance_[sort_idx]\n",
    "# features_name = X_df.columns[sort_idx]\n",
    "features_name = np.array(dataset.features)[sort_idx]\n",
    "\n",
    "features_importance_df = pd.DataFrame(features_importance_, index=features_name, columns=['Importance'])\n",
    "features_importance_df.index.name = 'Features'\n",
    "\n",
    "features_importance_df.to_csv('data/features_importance/oversampling/chi2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d9b8aa8d774518be7ebcfd06a2463a8035a66798fac49b1a363f570d2d8622e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
