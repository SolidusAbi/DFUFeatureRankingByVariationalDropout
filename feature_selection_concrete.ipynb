{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from dataset import FeaturesDataset\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class FeaturesDataset(Dataset):\n",
    "    def __init__(self, dataset_dir: str, normalize: bool = True):\n",
    "        super(FeaturesDataset, self).__init__()\n",
    "        X_df = pd.read_csv(os.path.join(dataset_dir, 'dfu_features_dataset_selected.csv'), index_col=0)\n",
    "        y_df = pd.read_csv(os.path.join(dataset_dir, 'dfu_labels_dataset.csv'), index_col=0)\n",
    "        \n",
    "        self.features = X_df.columns.to_list()\n",
    "        self.X = X_df.to_numpy().astype(np.float32)\n",
    "        self.y = y_df.to_numpy().ravel()\n",
    "\n",
    "        if normalize:\n",
    "            self.X = (self.X - self.X.mean(axis=0)) / self.X.std(axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = FeaturesDataset(dataset_dir='data/dataset', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concrete Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class LinearCD(nn.Linear):\n",
    "    r'''\n",
    "        Linear layer with Concrete Dropout regularization.\n",
    "\n",
    "        Code strongly inspired by: \n",
    "            https://github.com/danielkelshaw/ConcreteDropout/blob/master/condrop/concrete_dropout.py\n",
    "\n",
    "        Note the relationship between the weight regularizer (w_reg) and dropout regularization (drop_reg):\n",
    "        \n",
    "            w_reg/drop_reg = (l^2)/2 \n",
    "        \n",
    "        with prior lengthscale l (number of in_features). \n",
    "        \n",
    "        Note also that the factor of two should be ignored for cross-entropy loss, and used only for the\n",
    "        Euclidean loss.\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, bias=True, threshold=.1, init_min=.5, init_max=.5):\n",
    "        super(LinearCD, self).__init__(in_features, out_features, bias)        \n",
    "        logit_init_min = np.log(init_min) - np.log(1. - init_min)\n",
    "        logit_init_max = np.log(init_max) - np.log(1. - init_max)\n",
    "        \n",
    "        # The probability of deactive a neuron.\n",
    "        self.logit_p = nn.Parameter(torch.rand(in_features) * (logit_init_max - logit_init_min) + logit_init_min)\n",
    "        self.logit_threshold = np.log(threshold) - np.log(1. - threshold)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            return F.linear(self.concrete_bernoulli(x), self.weight, self.bias)\n",
    "\n",
    "        return F.linear(x, self.weight * (self.logit_p < self.logit_threshold).float(), self.bias) \n",
    "\n",
    "    def concrete_bernoulli(self, x):\n",
    "        eps = 1e-8\n",
    "        unif_noise = torch.cuda.FloatTensor(*x.size()).uniform_() if self.logit_p.is_cuda else torch.FloatTensor(*x.size()).uniform_()\n",
    "\n",
    "        p = torch.sigmoid(self.logit_p)\n",
    "        tmp = .1\n",
    "\n",
    "        drop_prob = (torch.log(p + eps) - torch.log((1-p) + eps) + torch.log(unif_noise + eps)\n",
    "        - torch.log((1. - unif_noise) + eps))\n",
    "        drop_prob = torch.sigmoid(drop_prob / tmp)\n",
    "\n",
    "        random_tensor = 1 - drop_prob\n",
    "        retain_prob = 1 - p # rescale factor typical for dropout\n",
    "\n",
    "        return torch.mul(x, random_tensor) #/ retain_prob\n",
    "\n",
    "    def reg(self):\n",
    "        tmp = .1\n",
    "        eps = 1e-6\n",
    "        p = torch.sigmoid(self.logit_p)\n",
    "        bernoulli = (torch.log(p + eps) - torch.log((1-p) + eps))\n",
    "        reg = 1 - torch.sigmoid(bernoulli / tmp)\n",
    "        return torch.sum(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,in_features: int, nb_features: int, threshold: float = .1):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        if threshold < 0. or threshold > 1.:\n",
    "            raise ValueError('threshold must be between 0 and 1')\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            LinearCD(in_features, nb_features, bias=False, threshold=threshold),\n",
    "            # nn.SiLU(),\n",
    "            nn.Linear(nb_features, nb_features//2),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_features//2, nb_features//4),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_features//4, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model, dataset, batch_size = 128, n_epochs=10):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # weighted sampler\n",
    "    samples = dataset.dataset.y[dataset.indices]\n",
    "    class_weight = [1/(samples == 0).sum(), 1/(samples == 1).sum()]\n",
    "    samples_weight = np.zeros(len(dataset))\n",
    "    samples_weight[samples == 0] = class_weight[0]\n",
    "    samples_weight[samples == 1] = class_weight[1]\n",
    "    \n",
    "    sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(dataset))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "    # loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    reg = 1e-6\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "            range(n_epochs),\n",
    "            leave=True,\n",
    "            unit=\"epoch\",\n",
    "            postfix={\"tls\": \"%.4f\" % 1},\n",
    "        )\n",
    "\n",
    "    modules = []\n",
    "    for i in model.modules():\n",
    "        if isinstance(i, LinearCD):\n",
    "            modules.append(i)\n",
    "\n",
    "    for _ in epoch_iterator:\n",
    "        reg = min(reg + 1e-5, 1e-2)\n",
    "        for idx, (inputs, targets) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            pred = model(inputs)\n",
    "\n",
    "            reg_value = 0\n",
    "            for module in modules:\n",
    "                reg_value += module.reg()\n",
    "\n",
    "            loss = criterion(pred, targets) + reg*reg_value\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                epoch_iterator.set_postfix(tls=\"%.4f\" % loss.item())\n",
    "    print(reg)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:15<00:00, 32.36epoch/s, tls=0.4971] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005000999999999965\n",
      "0.8157894736842105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:16<00:00, 30.04epoch/s, tls=0.3955] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005000999999999965\n",
      "0.868421052631579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:14<00:00, 34.70epoch/s, tls=0.5845] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005000999999999965\n",
      "0.868421052631579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:16<00:00, 29.68epoch/s, tls=0.4195] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005000999999999965\n",
      "0.8421052631578947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:15<00:00, 32.02epoch/s, tls=0.4839]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005000999999999965\n",
      "0.8918918918918919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_features = len(dataset.features)\n",
    "\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "features_importance = []\n",
    "model_accuracy = []\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    train_set = torch.utils.data.Subset(dataset, train_ids)\n",
    "    test_set = torch.utils.data.Subset(dataset, test_ids)\n",
    "\n",
    "    model = Model(n_features, 256, threshold=.2)\n",
    "    model = train(model, train_set, batch_size=32, n_epochs=500)\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "    for inputs, targets in test_loader:\n",
    "        pred = model(inputs.cuda())\n",
    "        pred = pred.argmax(dim=1)\n",
    "        y_pred.append(pred.cpu())\n",
    "        y_true.append(targets)\n",
    "\n",
    "    y_pred = torch.cat(y_pred)\n",
    "    y_true = torch.cat(y_true)\n",
    "    model_accuracy.append(accuracy_score(y_true, y_pred))\n",
    "    features_importance.append(torch.sigmoid(model.model[0].logit_p).cpu().detach().numpy())\n",
    "    print(model_accuracy[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.857325746799431\n"
     ]
    }
   ],
   "source": [
    "# Mean accuracy\n",
    "print('Mean accuracy: {}'.format(np.mean(model_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Features</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>R_LPA_min</th>\n",
       "      <td>0.170767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L_MPA_min</th>\n",
       "      <td>0.186285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_LCA_kurtosis</th>\n",
       "      <td>0.195982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Foot_ETD</th>\n",
       "      <td>0.202991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LCA_ETD</th>\n",
       "      <td>0.210169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_LCA_NRT_C1</th>\n",
       "      <td>0.730680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_LPA_NRT_C1</th>\n",
       "      <td>0.734289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_MCA_NRT_C5</th>\n",
       "      <td>0.735871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L_LCA_NRT_C4</th>\n",
       "      <td>0.738042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_NRT_C4</th>\n",
       "      <td>0.751901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Importance\n",
       "Features                  \n",
       "R_LPA_min         0.170767\n",
       "L_MPA_min         0.186285\n",
       "R_LCA_kurtosis    0.195982\n",
       "Foot_ETD          0.202991\n",
       "LCA_ETD           0.210169\n",
       "...                    ...\n",
       "R_LCA_NRT_C1      0.730680\n",
       "R_LPA_NRT_C1      0.734289\n",
       "R_MCA_NRT_C5      0.735871\n",
       "L_LCA_NRT_C4      0.738042\n",
       "R_NRT_C4          0.751901\n",
       "\n",
       "[125 rows x 1 columns]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_importance_ = torch.tensor(features_importance).mean(axis=0)\n",
    "features_score, index = features_importance_.sort()\n",
    "features_names = np.array(dataset.features)[index.cpu()]\n",
    "\n",
    "features_importance_df = pd.DataFrame(features_importance_[index], index=features_names, columns=['Importance'])\n",
    "features_importance_df.index.name = 'Features'\n",
    "\n",
    "features_importance_df.to_csv('data/features_importance/concrete_dropout.csv')\n",
    "\n",
    "features_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0847, 0.1017, 0.1022, 0.1051, 0.1123, 0.1146, 0.1263, 0.1267, 0.1846,\n",
       "        0.1909, 0.2108, 0.2165, 0.2478, 0.2855, 0.2923, 0.2928, 0.2941, 0.3037,\n",
       "        0.3215, 0.3799, 0.4331, 0.4369, 0.4385, 0.4409, 0.4453, 0.4453, 0.4460,\n",
       "        0.4611, 0.4654, 0.4659, 0.4789, 0.4797, 0.4970, 0.5022, 0.5225, 0.5239,\n",
       "        0.5382, 0.5589, 0.5632, 0.5657, 0.5786, 0.5879, 0.5879, 0.5930, 0.5967,\n",
       "        0.6033, 0.6083, 0.6136, 0.6141, 0.6180, 0.6206, 0.6209, 0.6213, 0.6264,\n",
       "        0.6300, 0.6308, 0.6333, 0.6337, 0.6349, 0.6353, 0.6376, 0.6388, 0.6423,\n",
       "        0.6437, 0.6503, 0.6540, 0.6541, 0.6548, 0.6601, 0.6609, 0.6668, 0.6722,\n",
       "        0.6736, 0.6766, 0.6773, 0.6806, 0.6831, 0.6836, 0.6846, 0.6890, 0.6890,\n",
       "        0.6897, 0.6913, 0.6919, 0.6948, 0.6953, 0.6994, 0.7031, 0.7032, 0.7057,\n",
       "        0.7068, 0.7071, 0.7076, 0.7111, 0.7137, 0.7147, 0.7185, 0.7189, 0.7221,\n",
       "        0.7226, 0.7253, 0.7287, 0.7300, 0.7322, 0.7332, 0.7348, 0.7379, 0.7406,\n",
       "        0.7406, 0.7470, 0.7491, 0.7540, 0.7544, 0.7569, 0.7589, 0.7619, 0.7647,\n",
       "        0.7675, 0.7713, 0.7754, 0.7916, 0.7948, 0.7960, 0.7961, 0.8046])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:['L_LPA_std' 'R_LPA_min' 'Foot_ETD' 'L_MPA_min' 'L_MPA_std' 'LPA_ETD'\n",
      " 'L_kurtosis' 'R_MPA_NRT_C2' 'L_NRT_C5' 'MPA_ETD' 'L_MCA_std'\n",
      " 'L_LPA_NRT_C5' 'R_LPA_NRT_C7' 'R_LPA_std' 'L_MPA_HSE' 'L_LCA_kurtosis'\n",
      " 'R_kurtosis' 'R_MCA_NRT_C4' 'R_MPA_skew' 'L_NRT_C4' 'L_LPA_NRT_C4'\n",
      " 'L_NRT_C7' 'R_HSE' 'L_NRT_C3' 'R_MCA_HSE' 'R_LCA_NRT_C4' 'R_NRT_C6'\n",
      " 'R_LCA_kurtosis' 'R_MCA_skew' 'L_MCA_skew' 'R_MPA_NRT_C7' 'R_MCA_std'\n",
      " 'R_LPA_NRT_C1' 'R_std' 'L_LCA_HSE' 'R_MPA_NRT_C3' 'R_MPA_HSE' 'MCA_ETD'\n",
      " 'L_MPA_kurtosis' 'R_LPA_HSE' 'LCA_ETD' 'R_LPA_kurtosis' 'R_MCA_kurtosis'\n",
      " 'L_HSE' 'L_LPA_kurtosis' 'L_LCA_NRT_C2' 'L_LPA_NRT_C6' 'L_MCA_NRT_C1'\n",
      " 'R_MPA_kurtosis' 'R_NRT_C4' 'R_LCA_HSE' 'R_TCI' 'R_MCA_NRT_C5'\n",
      " 'L_MPA_NRT_C4' 'L_LPA_NRT_C1' 'L_MPA_NRT_C5' 'R_LCA_NRT_C3'\n",
      " 'R_MCA_NRT_C6' 'L_LCA_NRT_C3' 'L_MPA_NRT_C2' 'L_LPA_NRT_C7'\n",
      " 'R_LCA_NRT_C5' 'R_NRT_C2' 'R_MPA_NRT_C4' 'R_NRT_C5' 'L_MCA_NRT_C4'\n",
      " 'R_LPA_skew' 'L_MCA_NRT_C3' 'L_LPA_skew' 'R_skew' 'R_MPA_max' 'L_MPA_max'\n",
      " 'L_MCA_NRT_C0' 'R_NRT_C7' 'R_MPA_NRT_C6' 'R_LCA_NRT_C6' 'R_MCA_NRT_C2'\n",
      " 'R_LPA_NRT_C4' 'R_LPA_NRT_C2' 'R_MPA_NRT_C5' 'L_LCA_max' 'R_LCA_NRT_C7'\n",
      " 'L_MPA_skew' 'L_LPA_HSE' 'L_MPA_NRT_C3' 'L_MCA_min' 'L_LCA_NRT_C7'\n",
      " 'R_MCA_NRT_C7' 'R_MPA_std' 'L_MCA_NRT_C7' 'L_NRT_C2' 'R_MCA_NRT_C3'\n",
      " 'R_NRT_C3' 'L_MCA_NRT_C2' 'L_TCI' 'R_MCA_NRT_C0' 'R_LCA_skew'\n",
      " 'R_LPA_NRT_C6' 'L_LCA_NRT_C6' 'L_LCA_NRT_C1' 'L_LCA_std' 'L_LPA_NRT_C3'\n",
      " 'L_LPA_ET' 'L_skew' 'L_LCA_NRT_C5' 'R_LCA_NRT_C2' 'L_LCA_NRT_C4'\n",
      " 'L_MCA_kurtosis' 'R_LPA_NRT_C5' 'L_MCA_HSE' 'L_NRT_C6' 'L_LCA_skew'\n",
      " 'R_LCA_std' 'L_MPA_NRT_C7' 'L_LPA_NRT_C2' 'L_MPA_NRT_C6' 'R_LPA_NRT_C3'\n",
      " 'L_MCA_NRT_C6' 'L_std' 'R_LCA_NRT_C1' 'R_MPA_ET' 'L_MCA_NRT_C5'\n",
      " 'L_LPA_NRT_C0' 'R_LCA_mean' 'L_MCA_mean']\n",
      "Features Score:tensor([0.9927, 0.9817, 0.9795, 0.9496, 0.9435, 0.9122, 0.4585, 0.4428, 0.4418,\n",
      "        0.4364, 0.4349, 0.4334, 0.4264, 0.4222, 0.4185, 0.4169, 0.4161, 0.4128,\n",
      "        0.4128, 0.4114, 0.4106, 0.4081, 0.4077, 0.4066, 0.4031, 0.4027, 0.4024,\n",
      "        0.4019, 0.4010, 0.3991, 0.3976, 0.3966, 0.3943, 0.3936, 0.3928, 0.3910,\n",
      "        0.3910, 0.3877, 0.3869, 0.3864, 0.3859, 0.3857, 0.3850, 0.3819, 0.3809,\n",
      "        0.3803, 0.3788, 0.3788, 0.3786, 0.3717, 0.3714, 0.3668, 0.3664, 0.3602,\n",
      "        0.3601, 0.3592, 0.3582, 0.3565, 0.3564, 0.3553, 0.3549, 0.3531, 0.3521,\n",
      "        0.3510, 0.3501, 0.3495, 0.3467, 0.3459, 0.3427, 0.3425, 0.3423, 0.3414,\n",
      "        0.3391, 0.3389, 0.3368, 0.3353, 0.3308, 0.3303, 0.3221, 0.3214, 0.3213,\n",
      "        0.3211, 0.3208, 0.3171, 0.3152, 0.3139, 0.3133, 0.3102, 0.3044, 0.2996,\n",
      "        0.2955, 0.2942, 0.2924, 0.2890, 0.2888, 0.2886, 0.2865, 0.2820, 0.2800,\n",
      "        0.2790, 0.2749, 0.2749, 0.2732, 0.2703, 0.2600, 0.2573, 0.2533, 0.2463,\n",
      "        0.2328, 0.2154, 0.2070, 0.1998, 0.1990, 0.1981, 0.1937, 0.1901, 0.1894,\n",
      "        0.1877, 0.1621, 0.1611, 0.1089, 0.1062, 0.0867, 0.0863, 0.0860])\n"
     ]
    }
   ],
   "source": [
    "features_importance_ = np.array(features_importance).mean(axis=0)\n",
    "\n",
    "features_score, index = torch.tensor(features_importance_).sort()\n",
    "\n",
    "features_names = dataset.features\n",
    "\n",
    "print('Features:{}'.format(np.array(features_names)[index]))\n",
    "print('Features Score:{}'.format(1-features_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False, False, False,  True, False,  True, False,\n",
      "        False, False,  True,  True, False,  True, False,  True,  True, False,\n",
      "         True,  True, False, False,  True,  True, False,  True, False, False,\n",
      "        False, False])\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "x, y = next(iter(loader))\n",
    "\n",
    "threshold = .1\n",
    "model.model[0].logit_threshold = torch.tensor(np.log(threshold) - np.log(1. - threshold))\n",
    "model.eval()\n",
    "torch.argmax(torch.softmax(model(x.cuda()), dim=1), dim=1)\n",
    "print(y==torch.argmax(torch.softmax(model(x.cuda()), dim=1), dim=1).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype='<U14')"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(dataset.features)[(torch.sigmoid(model.model[0].logit_p)<0.1).cpu()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1672, device='cuda:0', grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(model.model[0].logit_p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d9b8aa8d774518be7ebcfd06a2463a8035a66798fac49b1a363f570d2d8622e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
